{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fdvqAdpt73a"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai tiktoken cohere arxiv duckduckgo-search langchainhub -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hExXPckwxiBx",
        "outputId": "05132c25-d683-49c0-db5d-f08bcad7d4f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv())\n",
        "# import getpass\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwZJqiQSuv0I"
      },
      "source": [
        "## LangChain ReAct Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKDixP3SyM4o"
      },
      "source": [
        "### Base LLM Powering RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B_8HDZr9unxL"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_models.openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gEOGBxy3xz-N"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(\n",
        "        content=\"Hello, how are you?\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgZQpCrYyH4g",
        "outputId": "5d73c8c0-2a88-4c04-fa58-208770c3fd60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 13, 'total_tokens': 46}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_94f711dcf6', 'finish_reason': 'stop', 'logprobs': None}, id='run-201e17cc-04de-4401-90cd-cc9eaa6491b2-0')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au-2Ks6kyQop"
      },
      "source": [
        "### Tool Belt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mJkFIJHpyMQD"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "tools = load_tools([\"arxiv\", \"ddg-search\"], llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSV_xhhm0HI4"
      },
      "source": [
        "### LCEL Agent Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qemUUWpou2Je"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/react-json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW1JX14k0BUH",
        "outputId": "1da93a44-1e60-4f91-f9fe-77747c567951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react-json', 'lc_hub_commit_hash': '669cf4d6988c3b8994a8189edb3891e07948e1c0abfd500823914548c53afa7c'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \"action\" field are: {tool_names}\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n\\n{agent_scratchpad}'))])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lizocS910L0H"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.render import render_text_description\n",
        "\n",
        "prompt = prompt.partial(\n",
        "    tools=render_text_description(tools),\n",
        "    tool_names=\", \".join([t.name for t in tools]),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], partial_variables={'tools': 'arxiv: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.', 'tool_names': 'arxiv, duckduckgo_search'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react-json', 'lc_hub_commit_hash': '669cf4d6988c3b8994a8189edb3891e07948e1c0abfd500823914548c53afa7c'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \"action\" field are: {tool_names}\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n\\n{agent_scratchpad}'))])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OxOn0uaC0PqX"
      },
      "outputs": [],
      "source": [
        "llm_with_stop = llm.bind(stop=[\"\\nObservation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3snqga5S0Sb5"
      },
      "outputs": [],
      "source": [
        "from langchain.agents.format_scratchpad import format_log_to_str\n",
        "from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser\n",
        "\n",
        "agent = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm_with_stop\n",
        "    | ReActJsonSingleInputOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LHBkdVc90YcH"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPuZ7yxP0nwe",
        "outputId": "d7ffbe79-a0ea-4e7b-9782-34bd2415fe44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Retrieval Augmented Generation (RAG) is a concept in the field of natural language processing and machine learning. It refers to a methodology where a generative model is augmented with a retrieval component to enhance its performance, typically by providing additional context or information. To provide a detailed and accurate explanation, I can search for scientific articles on arxiv.org that discuss Retrieval Augmented Generation.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"arxiv\",\n",
            "  \"action_input\": \"Retrieval Augmented Generation\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2022-02-13\n",
            "Title: A Survey on Retrieval-Augmented Text Generation\n",
            "Authors: Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu\n",
            "Summary: Recently, retrieval-augmented text generation attracted increasing attention\n",
            "of the computational linguistics community. Compared with conventional\n",
            "generation models, retrieval-augmented text generation has remarkable\n",
            "advantages and particularly has achieved state-of-the-art performance in many\n",
            "NLP tasks. This paper aims to conduct a survey about retrieval-augmented text\n",
            "generation. It firstly highlights the generic paradigm of retrieval-augmented\n",
            "generation, and then it reviews notable approaches according to different tasks\n",
            "including dialogue response generation, machine translation, and other\n",
            "generation tasks. Finally, it points out some important directions on top of\n",
            "recent methods to facilitate future research.\n",
            "\n",
            "Published: 2023-12-09\n",
            "Title: Context Tuning for Retrieval Augmented Generation\n",
            "Authors: Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi\n",
            "Summary: Large language models (LLMs) have the remarkable ability to solve new tasks\n",
            "with just a few examples, but they need access to the right tools. Retrieval\n",
            "Augmented Generation (RAG) addresses this problem by retrieving a list of\n",
            "relevant tools for a given task. However, RAG's tool retrieval step requires\n",
            "all the required information to be explicitly present in the query. This is a\n",
            "limitation, as semantic search, the widely adopted tool retrieval method, can\n",
            "fail when the query is incomplete or lacks context. To address this limitation,\n",
            "we propose Context Tuning for RAG, which employs a smart context retrieval\n",
            "system to fetch relevant information that improves both tool retrieval and plan\n",
            "generation. Our lightweight context retrieval model uses numerical,\n",
            "categorical, and habitual usage signals to retrieve and rank context items. Our\n",
            "empirical results demonstrate that context tuning significantly enhances\n",
            "semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\n",
            "context retrieval and tool retrieval tasks respectively, and resulting in an\n",
            "11.6% increase in LLM-based planner accuracy. Additionally, we show that our\n",
            "proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\n",
            "outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\n",
            "plan generation, even after tool retrieval, reduces hallucination.\n",
            "\n",
            "Published: 2024-02-16\n",
            "Title: Corrective Retrieval Augmented Generation\n",
            "Authors: Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling\n",
            "Summary: Large language models (LLMs) inevitably exhibit hallucinations since the\n",
            "accuracy of generated texts cannot be secured solely by the parametric\n",
            "knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\n",
            "practicable complement to LLMs, it relies heavily on the relevance of retrieved\n",
            "documents, raising concerns about how the model behaves if retrieval goes\n",
            "wrong. To this end, we propose the Corrective Retrieval Augmented Generation\n",
            "(CRAG) to improve the robustness of generation. Specifically, a lightweight\n",
            "retrieval evaluator is designed to assess the overall quality of retrieved\n",
            "documents for a query, returning a confidence degree based on which different\n",
            "knowledge retrieval actions can be triggered. Since retrieval from static and\n",
            "limited corpora can only return sub-optimal documents, large-scale web searches\n",
            "are utilized as an extension for augmenting the retrieval results. Besides, a\n",
            "decompose-then-recompose algorithm is designed for retrieved documents to\n",
            "selectively focus on key information and filter out irrelevant information in\n",
            "them. CRAG is plug-and-play and can be seamlessly coupled with various\n",
            "RAG-based approaches. Experiments on four datasets covering short- and\n",
            "long-form generation tasks show that CRAG can significantly improve the\n",
            "performance of RAG-based approaches.\u001b[0m\u001b[32;1m\u001b[1;3mThe articles retrieved from arXiv provide a comprehensive overview of Retrieval Augmented Generation (RAG) and its applications in natural language processing. The first article is a survey that outlines the general paradigm of RAG and reviews notable approaches in different tasks such as dialogue response generation and machine translation. The second article discusses a method to improve RAG by using context tuning, which helps in retrieving more relevant information for the task at hand. The third article introduces Corrective Retrieval Augmented Generation (CRAG), which aims to improve the robustness of RAG by evaluating the quality of retrieved documents and using large-scale web searches to augment the retrieval results.\n",
            "\n",
            "Thought: With this information, I can now provide a detailed explanation of what Retrieval Augmented Generation is.\n",
            "\n",
            "Final Answer: Retrieval Augmented Generation (RAG) is a methodology in natural language processing that enhances generative models by incorporating a retrieval component. This retrieval component fetches relevant information or context from a data source, which is then used to inform and improve the generation process. RAG has been shown to achieve state-of-the-art performance in various NLP tasks, such as dialogue response generation and machine translation. The approach can be further improved by context tuning, which retrieves more pertinent context, or by corrective measures like CRAG, which assess the quality of retrieved documents and augment them with large-scale web searches to ensure the relevance and accuracy of the information used in the generation process.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is Retrieval Augmented Generation?',\n",
              " 'output': 'Retrieval Augmented Generation (RAG) is a methodology in natural language processing that enhances generative models by incorporating a retrieval component. This retrieval component fetches relevant information or context from a data source, which is then used to inform and improve the generation process. RAG has been shown to achieve state-of-the-art performance in various NLP tasks, such as dialogue response generation and machine translation. The approach can be further improved by context tuning, which retrieves more pertinent context, or by corrective measures like CRAG, which assess the quality of retrieved documents and augment them with large-scale web searches to ensure the relevance and accuracy of the information used in the generation process.'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\" : \"What is Retrieval Augmented Generation?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx--DyZD1Rbf",
        "outputId": "3339197f-2ddf-4e06-a81f-ab3355cb3610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: To find out the current quarterback of the Denver Broncos, I can use a search engine to get the most recent information.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"duckduckgo_search\",\n",
            "  \"action_input\": \"current QB of the Denver Broncos\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3mWith less than a month to go until the NFL Draft kicks off, the Denver Broncos are still stuck with their looming quarterback question of who will be starting under center for the first year post ... New Broncos starting quarterback is married, a father and has backed up Tom Brady, Cam Newton, Jimmy Garoppolo - and Russell Wilson in his career. Credit: AP Photo/David Zalubowski. Denver ... ENGLEWOOD, Colo. -- The quarterback question is the bottom line of the Denver Broncos' offseason. Coach Sean Payton benched Russell Wilsonwith two games remaining in the 2023 season. The five-year ... Tennessee Titans quarterback Ryan Tannehill (17) throws during the first half of an NFL football game against the Denver Broncos, Monday, Sept. 14, 2020, in Denver. (AP Photo/David Zalubowski ... State of the Broncos roster: QB is the biggest issue and question. What the Denver Broncos do at the quarterback position will drive the offseason and possible success on the field. When it comes ...\u001b[0m\u001b[32;1m\u001b[1;3mThought: The search results mention that the Denver Broncos have a quarterback question and that Coach Sean Payton benched Russell Wilson with two games remaining in the 2023 season. However, the search results do not explicitly state who the current starting quarterback is. I may need to refine the search to get a more direct answer.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"duckduckgo_search\",\n",
            "  \"action_input\": \"Denver Broncos starting quarterback 2023\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mJon Heath. follow. June 21, 2023 8:00 am MT. As we wrap up our position preview series for the Denver Broncos ahead of training camp and the 2023 season, we're down to the final — and most ... How he fared in Denver: Wilson, 34, has been the most stable starter for the Broncos since Peyton Manning retired. As of Nov. 20, 2023, he has started 25 games for Denver, completing 63.7% of his ... New Broncos starting quarterback is married, a father and has backed up Tom Brady, Cam Newton, Jimmy Garoppolo - and Russell Wilson in his career. Credit: AP Photo/David Zalubowski. Denver ... The Broncos are about to find out what they have in Jarrett Stidham, the up-until-now backup Denver quarterback who will be starting in the final two games of the 2023 season. Latest U.S. March 4, 2024 at 4:20 p.m. The Russell Wilson era ended on Monday after the Broncos told the quarterback on that they plan on releasing him after two seasons in Denver. Jarrett Stidham, who took ...\u001b[0m\u001b[32;1m\u001b[1;3mThe search results indicate that Jarrett Stidham started in the final two games of the 2023 season for the Denver Broncos. Additionally, it is mentioned that the Broncos plan on releasing Russell Wilson after two seasons in Denver. This suggests that Jarrett Stidham may be the current quarterback, but it is not explicitly stated if he is the designated starter for the upcoming season or if the team is still in the process of making a decision.\n",
            "\n",
            "Final Answer: Jarrett Stidham started in the final two games of the 2023 season for the Denver Broncos, and with the team planning to release Russell Wilson, it appears that Stidham may be the current quarterback. However, the exact status of the starting quarterback position for the upcoming season is not definitively stated in the search results.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Who is the current QB of the Denver Broncos?',\n",
              " 'output': 'Jarrett Stidham started in the final two games of the 2023 season for the Denver Broncos, and with the team planning to release Russell Wilson, it appears that Stidham may be the current quarterback. However, the exact status of the starting quarterback position for the upcoming season is not definitively stated in the search results.'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\" : \"Who is the current QB of the Denver Broncos?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr9GKW0uu0Hn"
      },
      "source": [
        "## OpenAI Assistant API\n",
        "\n",
        "Helper Functions for Assistant API taking from [this](https://gist.github.com/assafelovic/579822cd42d52d80db1e1c1ff82ffffd) gist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EX64v6iM2ZY-"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "  with DDGS() as ddgs:\n",
        "    results = [r for r in ddgs.text(query, max_results=5)]\n",
        "    return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "6HccR0kU2x3s",
        "outputId": "6191147f-eb96-4c2c-dd11-46cc5abd7898"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Lowry has been a part of the Jets organization since June 25, 2011, when he was selected in the third round, 67 th overall, after putting up 37 points in 36 games with the Swift Current Broncos of ...\\nAdam Lowry, who has been a Jet since 2011 when he was drafted 67th overall, is the new captain of the NHL team — its third since relocating to Winnipeg from Atlanta in 2011. Andrew Ladd served ...\\nLowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ...\\nThe Winnipeg Jets will have a captain for the 2023-24 season. ... Although it will be his first time serving as a team captain since his final year with the Swift Current Broncos in 2012-13, Lowry ...\\nWinnipeg played without a captain last season after stripping Blake Wheeler of the title in September. Lowry is entering his 10th season with the Jets, who drafted him in the third round in 2011.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is the current captain of the Winnipeg Jets?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NfEuZCCZ29L0"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "\n",
        "def arxiv_search(query):\n",
        "  results = []\n",
        "\n",
        "  search = arxiv.Search(\n",
        "      query = query,\n",
        "      max_results = 5,\n",
        "      sort_by = arxiv.SortCriterion.Relevance\n",
        "  )\n",
        "\n",
        "  for result in arxiv.Client().results(search):\n",
        "    results.append(result)\n",
        "\n",
        "  return \"\\n\".join(result.summary for result in results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "pGwyTXP03JgM",
        "outputId": "7be53780-8444-45cd-e315-9d452d3b57d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Existing research on response generation for chatbot focuses on \\\\textbf{First\\nResponse Generation} which aims to teach the chatbot to say the first response\\n(e.g. a sentence) appropriate to the conversation context (e.g. the user's\\nquery). In this paper, we introduce a new task \\\\textbf{Second Response\\nGeneration}, termed as Improv chat, which aims to teach the chatbot to say the\\nsecond response after saying the first response with respect the conversation\\ncontext, so as to lighten the burden on the user to keep the conversation\\ngoing. Specifically, we propose a general learning based framework and develop\\na retrieval based system which can generate the second responses with the\\nusers' query and the chatbot's first response as input. We present the approach\\nto building the conversation corpus for Improv chat from public forums and\\nsocial networks, as well as the neural networks based models for response\\nmatching and ranking. We include the preliminary experiments and results in\\nthis paper. This work could be further advanced with better deep matching\\nmodels for retrieval base systems or generative models for generation based\\nsystems as well as extensive evaluations in real-life applications.\\nThe goal of this dissertation is to investigate the PANS model capabilities\\nin providing significant improvement over RANS predictions at slightly higher\\ncomputational expense and producing LES quality results at significantly lower\\ncomputational cost. The objectives of this study are: (i) investigate the model\\nfidelity at a fixed level of scale resolution (Generation1-PANS/G1-PANS) for\\nsmooth surface separation, (ii) Derive the PANS closure model in regions of\\nresolution variation (Generation2-PANS/G2-PANS), and (iii) Validate G2-PANS\\nmodel for attached and separated flows. The separated flows considered in this\\nstudy have been designated as critical benchmark flows by NASA CFD study group.\\n  The key contributions of this dissertation are summarized as follows. The\\nturbulence closure model of varying resolution, G2-PANS, is developed by\\nderiving mathematically-consistent commutation residues and using energy\\nconservation principles. The log-layer recovery and accurate computation of\\nReynolds stress anisotropy is accomplished by transitioning from steady RANS to\\nscaled resolved simulations using the G2-PANS model. Finally, several\\nsmooth-separation flows on the NASA turbulence website have been computed with\\nhigh degree of accuracy at a significantly reduced computational effort over\\nLES using the G1-PANS and G2-PANS models.\\nWeighted Castelnuovo-Mumford Regularity and Weighted Global GenerationWe\\nintroduce and study a notion of Castelnuovo-Mumford regularity suitable for\\nweighted projective spaces.\\nMachine learning models typically suffer from the domain shift problem when\\ntrained on a source dataset and evaluated on a target dataset of different\\ndistribution. To overcome this problem, domain generalisation (DG) methods aim\\nto leverage data from multiple source domains so that a trained model can\\ngeneralise to unseen domains. In this paper, we propose a novel DG approach\\nbased on \\\\emph{Deep Domain-Adversarial Image Generation} (DDAIG). Specifically,\\nDDAIG consists of three components, namely a label classifier, a domain\\nclassifier and a domain transformation network (DoTNet). The goal for DoTNet is\\nto map the source training data to unseen domains. This is achieved by having a\\nlearning objective formulated to ensure that the generated data can be\\ncorrectly classified by the label classifier while fooling the domain\\nclassifier. By augmenting the source training data with the generated unseen\\ndomain data, we can make the label classifier more robust to unknown domain\\nchanges. Extensive experiments on four DG datasets demonstrate the\\neffectiveness of our approach.\\nHere we experiment with the use of information retrieval as an augmentation\\nfor pre-trained language models. The text corpus used in information retrieval\\ncan be viewed as form of episodic memory which grows over time. By augmenting\\nGPT 2.0 with information retrieval we achieve a zero shot 15% relative\\nreduction in perplexity on Gigaword corpus without any re-training. We also\\nvalidate our IR augmentation on an event co-reference task.\""
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arxiv_search(\"Retrieval Augmented Generation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BYwzl4fj4VCn"
      },
      "outputs": [],
      "source": [
        "ddg_function = {\n",
        "    \"name\" : \"duckduckgo_search\",\n",
        "    \"description\" : \"Answer non-technical questions. Do not use this tool for questions about Machine Learning.\",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tVtU3Co25CAB"
      },
      "outputs": [],
      "source": [
        "arxiv_function = {\n",
        "    \"name\" : \"arxiv_query\",\n",
        "    \"description\" : \"Answer technical questions about the Machine Learning domain.\",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Retrieval Augmented Generation'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TOJ0_o_70CMb"
      },
      "outputs": [
        {
          "ename": "InternalServerError",
          "evalue": "Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_04328f24bb7c70e52a5c84bff301360b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m----> 5\u001b[0m assistant \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massistants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuery Assistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a personal assistant. Use the provided functions to answer questions.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43marxiv_function\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mddg_function\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-1106-preview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/resources/beta/assistants/assistants.py:112\u001b[0m, in \u001b[0;36mAssistants.create\u001b[0;34m(self, model, description, file_ids, instructions, metadata, name, tools, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mCreate an assistant with a model and instructions.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI-Beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistants=v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/assistants\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43massistant_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssistantCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAssistant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/_base_client.py:1233\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1221\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1230\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1231\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    915\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/_base_client.py:998\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    997\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/_base_client.py:998\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    997\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.11/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1012\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1013\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1016\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1017\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1021\u001b[0m )\n",
            "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_04328f24bb7c70e52a5c84bff301360b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Query Assistant\",\n",
        "    instructions=\"You are a personal assistant. Use the provided functions to answer questions.\",\n",
        "    tools=[\n",
        "        {\"type\" : \"function\",\n",
        "         \"function\" : arxiv_function\n",
        "        },\n",
        "        {\"type\": \"function\",\n",
        "         \"function\" : ddg_function\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4-1106-preview\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFjuoeNS6FgW",
        "outputId": "13c79541-9185-4f62-dc1d-8a13c402ce3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant ID: asst_zKGTZPDZ1vwUCy1t3COqGe7Y\n"
          ]
        }
      ],
      "source": [
        "assistant_id = assistant.id\n",
        "\n",
        "print(f\"Assistant ID: {assistant_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYpML9_05pTW"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()\n",
        "user_input = \"What is the capital of Ottawa?\"\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=user_input,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtjsJYk65vny"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ZZ8vE_6O2m"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_run_completion(thread_id, run_id):\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n",
        "        print(f\"Current run status: {run.status}\")\n",
        "        if run.status in ['completed', 'failed', 'requires_action']:\n",
        "            return run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfgPscjY7EMQ",
        "outputId": "f1322946-f955-4ff0-86c8-efaf3e8e0636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current run status: requires_action\n"
          ]
        }
      ],
      "source": [
        "run = wait_for_run_completion(thread.id, run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0nFFepT6YUK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def submit_tool_outputs(thread_id, run_id, tools_to_call):\n",
        "    tool_output_array = []\n",
        "    for tool in tools_to_call:\n",
        "        output = None\n",
        "        tool_call_id = tool.id\n",
        "        function_name = tool.function.name\n",
        "        function_args = tool.function.arguments\n",
        "\n",
        "        if function_name == \"duckduckgo_search\":\n",
        "            print(\"Consulting Duck Duck Go...\")\n",
        "            output = duckduckgo_search(query=json.loads(function_args)[\"query\"])\n",
        "\n",
        "        if function_name == \"arxiv_query\":\n",
        "            print(\"Consulting Arxiv...\")\n",
        "            output = arxiv_search(query=json.loads(function_args)[\"query\"])\n",
        "\n",
        "        if output:\n",
        "            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n",
        "\n",
        "    print(tool_output_array)\n",
        "\n",
        "    return client.beta.threads.runs.submit_tool_outputs(\n",
        "        thread_id=thread_id,\n",
        "        run_id=run_id,\n",
        "        tool_outputs=tool_output_array\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U53cuas26pAm",
        "outputId": "be300449-b63b-4370-b4fd-814f5618eed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consulting Duck Duck Go...\n",
            "[{'tool_call_id': 'call_vBWBxJPTMBe5NNxkJj7Pyd0v', 'output': 'Gradient descent is a first-order optimization algorithm for finding a local minimum of a differentiable function. It steps in the direction of the negative gradient of the function at the current point, which is the steepest descent direction. It is useful for minimizing the cost or loss function and has various extensions and applications in machine learning and data mining.\\nLearn how gradient descent is a general-purpose algorithm that numerically estimates where a function outputs its lowest values. See examples of how to use it for multivariable functions, polynomials, and machine learning problems.\\nGradient descent is an optimization algorithm that trains machine learning models and neural networks by minimizing errors between predicted and actual results. Learn how it works, its types, challenges and applications with examples from IBM.\\nLearn the basics of Gradient Descent, an optimization algorithm used in machine/deep learning to minimize the cost/loss function. Explore the intuition, strategies, advantages and disadvantages of different types of Gradient Descent, such as momentum, NAG and stochastic.\\nThe gradient method, also called steepest descent method, is a method used in Numerics to solve general Optimization problems. Here one starts (at the example of a minimization problem) from an approximate value. From this one proceeds in the direction of the negative gradient (which indicates the direction of steepest descent from this ...'}]\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n"
          ]
        }
      ],
      "source": [
        "if run.status == 'requires_action':\n",
        "    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n",
        "    run = wait_for_run_completion(thread.id, run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r2iKN6K6wgc"
      },
      "outputs": [],
      "source": [
        "def print_messages_from_thread(thread_id):\n",
        "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
        "    for msg in messages:\n",
        "        print(f\"{msg.role}: {msg.content[0].text.value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxNyAJ0r6u1W",
        "outputId": "8f1b4bc6-5673-4963-ced3-ec31b4333e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: Gradient descent is a first-order optimization algorithm used to find the local minimum of a differentiable function. It operates by iteratively moving towards the direction of the steepest descent as defined by the negative of the gradient. The process involves taking steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.\n",
            "\n",
            "Gradient descent is particularly useful in machine learning and neural networks, where it is employed to minimize the cost or loss function—the measure of the difference between the actual and predicted outputs. By minimizing this error, the model's parameters are optimized during training.\n",
            "\n",
            "The algorithm starts from an initial point and takes steps proportional to the negative gradient of the function at the current point, gradually moving towards the area where the function has its lowest value, which corresponds to the most optimal solution for the model.\n",
            "\n",
            "There are different types of gradient descent, including:\n",
            "\n",
            "- **Batch Gradient Descent**: It uses the entire training set to calculate the gradient of the cost function for each iteration.\n",
            "- **Stochastic Gradient Descent (SGD)**: It uses only a single example from the training set to calculate the gradient and update the parameters with each iteration.\n",
            "- **Mini-batch Gradient Descent**: It is a compromise between batch and stochastic gradient descent where the gradient is computed and the parameters are updated for a small batch of training examples.\n",
            "\n",
            "Each of these variations has advantages and challenges, and the choice of which one to use depends on the size of the training set and the nature of the problem. Gradient descent is also a subject of ongoing research and enhancement within the field of numerical optimization and machine learning.\n",
            "user: What is Gradient Descent?\n"
          ]
        }
      ],
      "source": [
        "print_messages_from_thread(thread.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQCciGPn_8sV"
      },
      "outputs": [],
      "source": [
        "def use_assistant(query):\n",
        "  thread = client.beta.threads.create()\n",
        "\n",
        "  message = client.beta.threads.messages.create(\n",
        "      thread_id=thread.id,\n",
        "      role=\"user\",\n",
        "      content=query,\n",
        "  )\n",
        "\n",
        "  print(\"Creating Assistant \")\n",
        "\n",
        "  run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        "  )\n",
        "\n",
        "  print(\"Querying OpenAI Assistant Thread.\")\n",
        "\n",
        "  run = wait_for_run_completion(thread.id, run.id)\n",
        "\n",
        "  if run.status == 'requires_action':\n",
        "    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n",
        "    run = wait_for_run_completion(thread.id, run.id)\n",
        "\n",
        "  print_messages_from_thread(thread.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9CYInQ0Er0B",
        "outputId": "45f5beaf-e9e4-42d9-8889-fa5423024269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: requires_action\n",
            "Consulting Arxiv...\n",
            "[{'tool_call_id': 'call_MAr2jwPt5OPhkXKQmCj4S5mn', 'output': \"We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\nThe growing body of political texts opens up new opportunities for rich\\ninsights into political dynamics and ideologies but also increases the workload\\nfor manual analysis. Automated speaker attribution, which detects who said what\\nto whom in a speech event and is closely related to semantic role labeling, is\\nan important processing step for computational text analysis. We study the\\npotential of the large language model family Llama 2 to automate speaker\\nattribution in German parliamentary debates from 2017-2021. We fine-tune Llama\\n2 with QLoRA, an efficient training strategy, and observe our approach to\\nachieve competitive performance in the GermEval 2023 Shared Task On Speaker\\nAttribution in German News Articles and Parliamentary Debates. Our results shed\\nlight on the capabilities of large language models in automating speaker\\nattribution, revealing a promising avenue for computational analysis of\\npolitical discourse and the development of semantic role labeling systems.\\nVisual instruction tuning has recently shown encouraging progress with\\nopen-source large multimodal models (LMM) such as LLaVA and MiniGPT-4. However,\\nmost existing studies of open-source LMM are performed using models with 13B\\nparameters or smaller. In this paper we present an empirical study of scaling\\nLLaVA up to 33B and 65B/70B, and share our findings from our explorations in\\nimage resolution, data mixing and parameter-efficient training methods such as\\nLoRA/QLoRA. These are evaluated by their impact on the multi-modal and language\\ncapabilities when completing real-world tasks in the wild.\\n  We find that scaling LMM consistently enhances model performance and improves\\nlanguage capabilities, and performance of LoRA/QLoRA tuning of LMM are\\ncomparable to the performance of full-model fine-tuning. Additionally, the\\nstudy highlights the importance of higher image resolutions and mixing\\nmultimodal-language data to improve LMM performance, and visual instruction\\ntuning can sometimes improve LMM's pure language capability. We hope that this\\nstudy makes state-of-the-art LMM research at a larger scale more accessible,\\nthus helping establish stronger baselines for future research. Code and\\ncheckpoints will be made public.\\nLow Rank Decomposition of matrix - splitting a large matrix into a product of\\ntwo smaller matrix offers a means for compression that reduces the parameters\\nof a model without sparsification, and hence delivering more speedup on modern\\nhardware. Moreover, unlike quantization, the compressed linear layers remain\\nfully differentiable and all the parameters trainable, while being able to\\nleverage the existing highly efficient kernels over floating point matrices. We\\nstudy the potential to compress Large Language Models (LLMs) for monolingual\\nCode generation via Low Rank Decomposition (LoRD) and observe that ranks for\\nthe linear layers in these models can be reduced by upto 39.58% with less than\\n1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to\\ncompress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with\\nminimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single\\nA100. The compressed models speeds up inference by up to 22.35% with just a\\nsingle line of change in code over huggingface's implementation with pytorch\\nbackend. Low Rank Decomposition (LoRD) models remain compatible with state of\\nthe art near-lossless quantization method such as SpQR, which allows leveraging\\nfurther compression gains of quantization. Lastly, QLoRA over Low Rank\\nDecomposition (LoRD) model further reduces memory requirements by as much as\\n21.2% over vanilla QLoRA while offering similar gains from parameter efficient\\nfine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new\\nparadigm for LLM compression.\\nWe propose a simple approach for memory-efficient adaptation of pretrained\\nlanguage models. Our approach uses an iterative algorithm to decompose each\\npretrained matrix into a high-precision low-rank component and a\\nmemory-efficient quantized component. During finetuning, the quantized\\ncomponent remains fixed and only the low-rank component is updated. We present\\nan integer linear programming formulation of the quantization component which\\nenables dynamic configuration of quantization parameters (e.g., bit-width,\\nblock size) for each matrix given an overall target memory budget. We further\\nexplore a data-aware version of the algorithm which uses an approximation of\\nthe Fisher information matrix to weight the reconstruction objective during\\nmatrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B)\\ndemonstrate that our low-rank plus quantized matrix decomposition approach\\n(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables\\nmore aggressive quantization. For example, on the OpenAssistant benchmark\\nLQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a\\nmodel finetuned with 4-bit QLoRA. When finetuned on a language modeling\\ncalibration dataset, LQ-LoRA can also be used for model compression; in this\\nsetting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when\\nincluding the low-rank components and requires 27GB of GPU memory) is\\ncompetitive with the original model in full precision.\"}]\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: QLoRA is an efficient fine-tuning approach designed to significantly reduce memory usage, enabling the fine-tuning of very large neural network models, such as a 65 billion parameter language model, on a single 48GB GPU while maintaining full 16-bit fine-tuning task performance. It is particularly relevant for adapting large pretrained language models like ChatGPT and others.\n",
            "\n",
            "QLoRA operates by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA), which are components that adjust the language model for a specific task without modifying the original pretrained weights.\n",
            "\n",
            "Key innovations include:\n",
            "- **4-bit NormalFloat (NF4):** A new data type optimized for normally distributed weights.\n",
            "- **Double quantization:** Reduces the average memory footprint by quantizing the quantization constants.\n",
            "- **Paged optimizers:** Manages memory spikes during training.\n",
            "\n",
            "QLoRA's effectiveness has been demonstrated in the study where it outperformed all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT in only 24 hours of fine-tuning on a single GPU. Additionally, QLoRA has also been applied successfully for tasks like automated speaker attribution in German parliamentary debates, revealing its potential in various domains including computational text analysis and semantic role labeling.\n",
            "\n",
            "The approach also includes a performance analysis across multiple datasets, model types, including LLaMA and T5, and model scales. QLoRA's fine-tuning on a small high-quality dataset leads to state-of-the-art results even with smaller models than the previous SOTA (State of the Art).\n",
            "\n",
            "Furthermore, the method can be combined with other compression techniques like Low Rank Decomposition (LoRD) to further reduce memory requirements and achieve additional gains from parameter-efficient fine-tuning.\n",
            "\n",
            "QLoRA's memory efficiency and performance advantages make it an important contribution to the field of machine learning, particularly in the context of training and fine-tuning large language models with constrained computational resources.\n",
            "user: What is QLoRA?\n"
          ]
        }
      ],
      "source": [
        "use_assistant(\"What is QLoRA?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytbKC3JqI1BK",
        "outputId": "9cf3f878-5713-40bd-af20-d66006d2837a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: requires_action\n",
            "Consulting Duck Duck Go...\n",
            "[{'tool_call_id': 'call_UyLHF92vEZGEHFEfSsyh9PmA', 'output': 'A meme is an amusing or interesting item (such as a captioned picture or video) or genre of items that is spread widely online especially through social media. It can also mean an idea, behavior, style, or usage that spreads from person to person within a culture. Learn more about the origin, history, and examples of memes from this dictionary entry.\\nA meme is a virally transmitted image or video with text that shares commentary on cultural, social, or current events. Learn the origins, examples, and categories of memes, from general humor to world events, and how to make your own with Meme Generator.\\nA meme is a cultural shorthand that portrays a particular concept or idea and is spread through online social platforms. It can be an image, video, or text with a new caption that evolves and changes over time. Learn the history, examples, and how to understand a meme from dancing babies to the Momo Challenge.\\nA meme (/ m iː m / MEEM) is an idea, behavior, or style that spreads by means of imitation from person to person within a culture and often carries symbolic meaning representing a particular phenomenon or theme. A meme acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted from one mind to another through writing, speech, gestures, rituals, or other ...\\nThe modern-day definition of a meme is a humorous image, video, piece of text, or GIF that is spread across the internet, typically on social media, and often with slight variations. Memes can be created by anyone and can be about anything, from current events, to mundane tasks, to pop culture references. The length of a meme varies.'}]\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: A meme is essentially an amusing or interesting item, such as a captioned picture or video, that spreads widely online, particularly through social media. It is also defined as:\n",
            "\n",
            "- An idea, behavior, style, or usage that propagates from person to person within a culture.\n",
            "- A virally-transmitted symbol or idea that shares commentary on cultural, social, or current events.\n",
            "- A cultural shorthand that portrays a particular concept or idea, often mutating and evolving as it spreads through social platforms.\n",
            "\n",
            "Memes can take various forms, including images, videos, and texts with captions that change as they spread. They serve as a unit for carrying cultural ideas, symbols, or practices and often carry symbolic meaning representing a particular phenomenon or theme. Anyone can create a meme about virtually anything, and they often reflect current events, cultural trends, or social commentary.\n",
            "user: What is a meme?\n"
          ]
        }
      ],
      "source": [
        "use_assistant(\"What is a meme?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
